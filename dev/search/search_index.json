{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Great Expectations Airflow Provider documentation","text":"<p>With the Great Expectations Airflow Provider, you can validate data directly from a DAG.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>If you're new to the Great Expectations Airflow Provider, check out the getting started page for guidance on which options best fit your use case and instructions on how to use them.</p>"},{"location":"#examples","title":"Examples","text":"<p>Explore examples of end-to-end configuration and usage.</p>"},{"location":"#migration-guide","title":"Migration Guide","text":"<p>If you used the legacy <code>GreatExpectationsOperator</code>, follow the migration guide to update your configuration to use one of the new Operators.</p>"},{"location":"#getting-help","title":"Getting help","text":"<p>Report bugs, questions, and feature requests in our ticket tracker.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>The Great Expectations Airflow Provider is an open-source project. Learn about its development process and about how you can contribute:</p> <ul> <li>Contributing guide</li> <li>GitHub repository</li> </ul>"},{"location":"#license","title":"License","text":"<p>To learn more about the terms and conditions for use, reproduction, and distribution, read the Apache License 2.0.</p>"},{"location":"examples/","title":"Examples","text":"<p>Visit the Great Expectations Airflow Provider repository to explore examples of end-to-end configuration and usage.</p>"},{"location":"examples/#exercise-an-example-dag","title":"Exercise an example DAG","text":"<p>The example DAGs can be exercised with the open-source Astro CLI or with the Airflow web UI.</p> <p>To exercise an example DAG with the open-source Astro CLI:</p> <ol> <li>Initialize a project with the Astro CLI.</li> <li>Copy the example DAG into the <code>dags/</code> folder of your Astro project.</li> <li>Copy the contents of the <code>include/</code> folder of this repository into the <code>include/</code> directory of your Astro project.</li> <li>Add the following to your <code>Dockerfile</code> to install the <code>airflow-provider-great-expectations</code> package:    <pre><code>RUN pip install --user airflow-provider-great-expectations\n</code></pre></li> <li>Start Docker.</li> <li>Run <code>astro dev start</code> to view the DAG on a local Airflow instance.</li> </ol> <p>To exercise an example DAG with the Airflow web UI:</p> <ol> <li>Add the example DAG to your <code>dags/</code> folder.</li> <li>Make the <code>include/</code> directory available in your environment.</li> <li>Go to the DAGs View in the Airflow UI.</li> <li>Find the example DAG in the list and click the play button next to it.</li> </ol>"},{"location":"getting-started/","title":"Getting started","text":"<p>Great Expectations (GX) is a framework for describing data using expressive tests and then validating that the data meets test criteria. Astronomer maintains the Great Expectations Airflow Provider to give users a convenient method for running validations directly from their DAGs. The Great Expectations Airflow Provider has three Operators to choose from, which vary in the amount of configuration they require and the flexibility they provide.</p> <ul> <li><code>GXValidateDataFrameOperator</code></li> <li><code>GXValidateBatchOperator</code></li> <li><code>GXValidateCheckpointOperator</code></li> </ul>"},{"location":"getting-started/#operator-use-cases","title":"Operator use cases","text":"<p>When deciding which Operator best fits your use case, consider the location of the data you are validating, whether or not you need external alerts or actions to be triggered by the Operator, and what Data Context you will use. When picking a Data Context, consider whether or not you need to view how results change over time.</p> <ul> <li>If your data is in memory as a Spark or Pandas DataFrame, we recommend using the <code>GXValidateDataFrameOperator</code>. This option requires only a DataFrame and your Expectations to create a validation result.</li> <li>If your data is not in memory, we recommend configuring GX to connect to it by defining a BatchDefinition with the <code>GXValidateBatchOperator</code>. This option requires a BatchDefinition and your Expectations to create a validation result.</li> <li>If you want to trigger actions based on validation results, use the <code>GXValidateCheckpointOperator</code>. This option supports all features of GX Core, so it requires the most configuration - you have to define a  Checkpoint, BatchDefinition, ExpectationSuite, and ValidationDefinition to get validation results.</li> </ul> <p>The Operators vary in which Data Contexts they support. All 3 Operators support Ephemeral and GX Cloud Data Contexts. Only the <code>GXValidateCheckpointOperator</code> supports the File Data Context.</p> <ul> <li>If the results are used only within the Airflow DAG by other tasks, we recommend using an Ephemeral Data Context. The serialized Validation Result will be available within the DAG as the task result, but will not persist externally for viewing the results across multiple runs. All 3 Operators support the Ephemeral Data Context.</li> <li>To persist and view results outside of Airflow, we recommend using a Cloud Data Context. Validation Results are automatically visible in the GX Cloud UI when using a Cloud Data Context, and the task result contains a link to the stored validation result. All 3 Operators support the Cloud Data Context.</li> <li>If you want to manage Validation Results yourself, use a File Data Context. With this option, Validation Results can be viewed in Data Docs. Only the <code>GXValidateCheckpointOperator</code> supports the File Data Context.</li> </ul>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python version 3.9 to 3.12</li> <li>Great Expectations version 1.3.11+</li> <li>Apache Airflow\u00ae version 2.1.0+</li> </ul>"},{"location":"getting-started/#assumed-knowledge","title":"Assumed knowledge","text":"<p>To get the most out of this getting started guide, make sure you have an understanding of:</p> <ul> <li>The basics of Great Expectations. See Try GX Core.</li> <li>Airflow fundamentals, such as writing DAGs and defining tasks. See Get started with Apache Airflow.</li> <li>Airflow Operators. See Operators 101.</li> <li>Airflow connections. See Managing your Connections in Apache Airflow.</li> </ul>"},{"location":"getting-started/#install-the-provider-and-dependencies","title":"Install the provider and dependencies","text":"<ol> <li>Install the provider.</li> </ol> <p><pre><code>pip install airflow-provider-great-expectations\n</code></pre> 2. (Optional) Install additional dependencies for the data sources you\u2019ll use. For example, to install the optional Snowflake dependency, use the following command.</p> <p><pre><code>pip install \"airflow-provider-great-expectations[snowflake]\"\n</code></pre>    The following backends are supported as optional dependencies:     - <code>athena</code>     - <code>azure</code>     - <code>bigquery</code>     - <code>gcp</code>     - <code>mssql</code>     - <code>postgresql</code>     - <code>s3</code>     - <code>snowflake</code>     - <code>spark</code></p>"},{"location":"getting-started/#configure-an-operator","title":"Configure an Operator","text":"<p>After deciding which Operator best fits your use case, follow the Operator-specific instructions below to configure it.</p>"},{"location":"getting-started/#data-frame-operator","title":"Data Frame Operator","text":"<ol> <li> <p>Import the Operator.</p> <pre><code>from great_expectations_provider.operators.validate_dataframe import (\n    GXValidateDataFrameOperator,\n)\n</code></pre> </li> <li> <p>Instantiate the Operator with required and optional parameters.</p> <pre><code>from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from pandas import DataFrame\n\ndef my_data_frame_configuration(): DataFrame:\n    import pandas as pd  # airflow best practice is to not import heavy dependencies in the top level\n    return pd.read_csv(my_data_file)\n\nmy_data_frame_operator = GXValidateDataFrameOperator(\n    task_id=\"my_data_frame_operator\",\n    configure_dataframe=my_data_frame_configuration,\n    expect=my_expectation_suite,\n)\n</code></pre> <ul> <li><code>task_id</code>: alphanumeric name used in the Airflow UI and GX Cloud.</li> <li><code>configure_dataframe</code>: function that returns a DataFrame to pass data to the Operator.</li> <li><code>expect</code>: either a single Expectation or an Expectation Suite to validate against your data.</li> <li><code>result_format</code> (optional): accepts <code>BOOLEAN_ONLY</code>, <code>BASIC</code>, <code>SUMMARY</code>, or <code>COMPLETE</code> to set the verbosity of returned Validation Results. Defaults to <code>SUMMARY</code>.</li> <li><code>context_type</code> (optional): accepts <code>ephemeral</code> or <code>cloud</code> to set the Data Context used by the Operator. Defaults to <code>ephemeral</code>, which does not persist results between runs. To save and view Validation Results in GX Cloud, use <code>cloud</code> and complete the additional Cloud Data Context configuration below.</li> </ul> </li> </ol> <p>For more details, explore this end-to-end code sample.</p> <ol> <li> <p>If you use a Cloud Data Context, create a free GX Cloud account to get your Cloud credentials and then set the following Airflow variables.</p> <ul> <li><code>GX_CLOUD_ACCESS_TOKEN</code></li> <li><code>GX_CLOUD_ORGANIZATION_ID</code></li> </ul> </li> </ol>"},{"location":"getting-started/#batch-operator","title":"Batch Operator","text":"<ol> <li> <p>Import the Operator.</p> <pre><code>from great_expectations_provider.operators.validate_batch import (\n    GXValidateBatchOperator,\n)\n</code></pre> </li> <li> <p>Instantiate the Operator with required and optional parameters.</p> <pre><code>my_batch_operator = GXValidateBatchOperator(\n    task_id=\"my_batch_operator\",\n    configure_batch_definition=my_batch_definition_function,\n    expect=my_expectation_suite,\n)\n</code></pre> <ul> <li><code>task_id</code>: alphanumeric name used in the Airflow UI and GX Cloud.</li> <li><code>configure_batch_definition</code>: function that returns a BatchDefinition to configure GX to read your data.</li> <li><code>expect</code>: either a single Expectation or an Expectation Suite to validate against your data.</li> <li><code>batch_parameters</code> (optional): dictionary that specifies a time-based Batch of data to validate your Expectations against. Defaults to the first valid Batch found, which is the most recent Batch (with default sort ascending) or the oldest Batch if the Batch Definition has been configured to sort descending.</li> <li><code>result_format</code> (optional): accepts <code>BOOLEAN_ONLY</code>, <code>BASIC</code>, <code>SUMMARY</code>, or <code>COMPLETE</code> to set the verbosity of returned Validation Results. Defaults to <code>SUMMARY</code>.</li> <li><code>context_type</code> (optional): accepts <code>ephemeral</code> or <code>cloud</code> to set the Data Context used by the Operator. Defaults to <code>ephemeral</code>, which does not persist results between runs. To save and view Validation Results in GX Cloud, use <code>cloud</code> and complete the additional Cloud Data Context configuration below.</li> </ul> </li> </ol> <p>For more details, explore this end-to-end code sample.</p> <ol> <li> <p>If you use a Cloud Data Context, create a free GX Cloud account to get your Cloud credentials and then set the following Airflow variables.</p> <ul> <li><code>GX_CLOUD_ACCESS_TOKEN</code></li> <li><code>GX_CLOUD_ORGANIZATION_ID</code></li> </ul> </li> </ol>"},{"location":"getting-started/#checkpoint-operator","title":"Checkpoint Operator","text":"<ol> <li> <p>Import the Operator.</p> <pre><code>from great_expectations_provider.operators.validate_checkpoint import (\n    GXValidateCheckpointOperator,\n)\n</code></pre> </li> <li> <p>Instantiate the Operator with required and optional parameters.</p> <pre><code>my_checkpoint_operator = GXValidateCheckpointOperator(\n    task_id=\"my_checkpoint_operator\",\n    configure_checkpoint=my_checkpoint_function,\n)\n</code></pre> <ul> <li><code>task_id</code>: alphanumeric name used in the Airflow UI and GX Cloud.</li> <li><code>configure_checkpoint</code>: function that returns a Checkpoint, which orchestrates a ValidationDefinition, BatchDefinition, and ExpectationSuite. The Checkpoint can also specify a Result Format and trigger actions based on Validation Results.</li> <li><code>batch_parameters</code> (optional): dictionary that specifies a time-based Batch of data to validate your Expectations against. Defaults to the first valid Batch found, which is the most recent Batch (with default sort ascending) or the oldest Batch if the Batch Definition has been configured to sort descending.</li> <li><code>context_type</code> (optional): accepts <code>ephemeral</code>, <code>cloud</code>, or <code>file</code> to set the Data Context used by the Operator. Defaults to <code>ephemeral</code>, which does not persist results between runs. To save and view Validation Results in GX Cloud, use <code>cloud</code> and complete the additional Cloud Data Context configuration below. To manage Validation Results yourself, use <code>file</code> and complete the additional File Data Context configuration below.</li> <li><code>configure_file_data_context</code> (optional): function that returns a FileDataContext. Applicable only when using a File Data Context. See the additional File Data Context configuration below for more information.</li> </ul> </li> </ol> <p>For more details, explore this end-to-end code sample.</p> <ol> <li> <p>If you use a Cloud Data Context, create a free GX Cloud account to get your Cloud credentials and then set the following Airflow variables.</p> <ul> <li><code>GX_CLOUD_ACCESS_TOKEN</code></li> <li><code>GX_CLOUD_ORGANIZATION_ID</code></li> </ul> </li> <li> <p>If you use a File Data Context, pass the <code>configure_file_data_context</code> parameter. This takes a function that returns a FileDataContext. By default, GX will write results in the configuration directory. If you are retrieving your FileDataContext from a remote location, you can yield the FileDataContext in the <code>configure_file_data_context</code> function and write the directory back to the remote after control is returned to the generator.</p> </li> </ol>"},{"location":"getting-started/#manage-data-source-credentials-with-airflow-connections","title":"Manage Data Source credentials with Airflow Connections","text":"<p>The Great Expectations Airflow Provider includes functions to retrieve connection credentials from other Airflow provider Connections. The following external Connections are supported:</p>"},{"location":"getting-started/#supported-connection-types","title":"Supported Connection Types","text":"Connection Type API Function External Provider Documentation Amazon Redshift <code>build_redshift_connection_string(conn_id, schema=None)</code> Amazon Provider MySQL <code>build_mysql_connection_string(conn_id, schema=None)</code> MySQL Provider Microsoft SQL Server <code>build_mssql_connection_string(conn_id, schema=None)</code> Microsoft SQL Server Provider PostgreSQL <code>build_postgres_connection_string(conn_id, schema=None)</code> PostgreSQL Provider Snowflake <code>build_snowflake_connection_string(conn_id, schema=None)</code> Snowflake Provider Snowflake (Key-based Auth) <code>build_snowflake_key_connection(conn_id, schema=None)</code> Snowflake Provider Google Cloud BigQuery <code>build_gcpbigquery_connection_string(conn_id, schema=None)</code> Google Cloud Provider SQLite <code>build_sqlite_connection_string(conn_id)</code> SQLite Provider AWS Athena <code>build_aws_connection_string(conn_id, schema=None, database=None, s3_path=None, region=None)</code> Amazon Provider"},{"location":"getting-started/#usage","title":"Usage","text":"<p>To use these functions, first install the Airflow Provider that maintains the connection you need, and use the Airflow UI to configure the Connection with your credentials. Then, import the function you need from <code>great_expectations_provider.hooks.external_connections</code> and use it within your <code>configure_batch_definition</code> or <code>configure_checkpoint</code> function.</p> <pre><code>from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom great_expectations_provider.common.external_connections import (\n   build_postgres_connection_string,\n)\n\nif TYPE_CHECKING:\n   from great_expectations.data_context import AbstractDataContext\n   from great_expectations.core.batch_definition import BatchDefinition\n\n\ndef configure_postgres_batch_definition(\n        context: AbstractDataContext,\n) -&gt; BatchDefinition:\n   task_id = \"example_task\"\n   table_name = \"example_table\"\n   postgres_conn_id = \"example_conn_id\"\n   return (\n      context.data_sources.add_postgres(\n         name=task_id,\n         connection_string=build_postgres_connection_string(\n            conn_id=postgres_conn_id\n         ),\n      )\n      .add_table_asset(\n         name=task_id,\n         table_name=table_name,\n      )\n      .add_batch_definition_whole_table(task_id)\n   )\n</code></pre>"},{"location":"getting-started/#add-the-configured-operator-to-a-dag","title":"Add the configured Operator to a DAG","text":"<p>After configuring an Operator, add it to a DAG. Explore our example DAGs, which have sample tasks that demonstrate Operator functionality.</p> <p>Note that the shape of the Validation Results depends on both the Operator type and whether or not you set the optional <code>result_format</code> parameter. - <code>GXValidateDataFrameOperator</code> and <code>GXValidateBatchOperator</code> return a serialized ExpectationSuiteValidationResult - <code>GXValidateCheckpointOperator</code> returns a CheckpointResult. - The included fields depend on the Result Format verbosity.</p>"},{"location":"getting-started/#run-the-dag","title":"Run the DAG","text":"<p>Trigger the DAG manually or run it on a schedule to start validating your expectations of your data.</p>"},{"location":"migration-guide/","title":"Migration guide","text":"<p>This guide will help you migrate from V0 to V1 of the Great Expectations Airflow Provider.</p> <p>Here is an overview of key differences between versions:</p> Provider version V0 V1 Operators <code>GreatExpectationsOperator</code> <code>GXValidateDataFrameOperator</code><code>GXValidateBatchOperator</code><code>GXValidateCheckpointOperator</code> GX version 0.18 and earlier 1.3.11 and later Data Contexts File EphemeralCloudFile (<code>GXValidateCheckpointOperator</code> only) Response handling By default, any Expectation failure raises an <code>AirflowException</code>. To override this behavior and continue running the pipeline even if tests fail, you can set the <code>fail_task_on_validation_failure</code> flag to <code>False</code>. Regardless of Expectation failure or success, a Validation Result is made available to subsequent tasks, which can decide what to do with the result. <p>For guidance on which Operator and Data Context best fit your needs, see Operator use cases. Note that while File Data Contexts are still supported with <code>GXValidateCheckpointOperator</code>, they require extra configuration and can be challenging to use when Airflow is running in a distributed environment. Most uses of the legacy <code>GreatExpectationsOperator</code> can now be satisfied with an Ephemeral or Cloud Data Context with either the <code>GXValidateDataFrameOperator</code> or the <code>GXValidateBatchOperator</code> to minimize configuration.</p>"},{"location":"migration-guide/#switch-to-the-new-data-frame-or-batch-operator-recommended","title":"Switch to the new Data Frame or Batch Operator (recommended)","text":"<p>The configuration options for the new <code>GXValidateDataFrameOperator</code> and <code>GXValidateBatchOperator</code> are streamlined compared to the old <code>GreatExpectationsOperator</code>. Switching to one of these doesn\u2019t involve translating existing configuration into new syntax line for line but rather paring back to a more minimal configuration.</p> <ul> <li>See getting started for an overview of required configuration.</li> <li>Explore examples of end-to-end configuration and usage.</li> </ul>"},{"location":"migration-guide/#migrate-to-the-new-checkpoint-operator","title":"Migrate to the new Checkpoint Operator","text":"<p>If you want to update your existing <code>GreatExpectationsOperator</code> configuration to use the new <code>GXValidateCheckpointOperator</code> with a File Data Context, follow these steps:</p> <ol> <li> <p>Update your GX project to be compatible with GX V1 by following the GX V0 to V1 Migration Guide.</p> </li> <li> <p>Write a function that instantiates and returns your File Data Context.</p> </li> </ol> <p>Here is a basic example for running Airflow in a stable file system where you can rely on your GX project being discoverable at the same place over multiple runs. This is important because GX will write Validation Results back to the project directory.</p> <pre><code>```\nimport great_expectatations as gx\nfrom great_expectations.data_context import FileDataContext\n\ndef configure_file_data_context() -&gt; FileDataContext:\n    return gx.get_context(\n        mode=\"file\",\n        project_root_dir=\"./path_to_your_existing_project\"\n    )\n```\n</code></pre> <p>Here's a more advanced example for running Airflow in an environment where the underlying file system is not stable. The steps here are as follows:     - fetch your GX project     - load the context     - yield the context to the Operator     - after the Operator has finished, write your project configuration back to the remote</p> <p>Be aware that you are responsible for managing concurrency, in the case that multiple tasks are reading and writing back to the remote simultaneously.</p> <pre><code>```\nimport great_expectatations as gx\nfrom great_expectations.data_context import FileDataContext\n\ndef configure_file_data_context() -&gt; FileDataContext:\n    # load your GX project from its remote source\n    yield gx.get_context(\n        mode=\"file\",\n        project_root_dir=\"./path_to_the_local_copy_of_your_existing_project\"\n    )\n    # write your GX project back to its remote source\n```\n</code></pre> <ol> <li> <p>Write a function that returns your Checkpoint.</p> <pre><code>from great_expectations import Checkpoint\nfrom great_expectations.data_context import AbstractDataContext\n\ndef configure_checkpoint(context: AbstractDataContext) -&gt; Checkpoint:\n    return context.checkpoints.get(name=\"&lt;YOUR CHECKPOINT NAME&gt;\")\n</code></pre> </li> <li> <p>See getting started for more information about required and optional configuration.</p> </li> <li>Explore examples of end-to-end configuration and usage.</li> </ol>"},{"location":"migration-guide/#migrate-connections","title":"Migrate Connections","text":"<p>The legacy Great Expectations Airflow Provider accepted a <code>conn_id</code> argument, which would attempt to retrieve credentials from other Airflow provider Connections. That logic is now available as provider-specific functions in <code>great_expectations_provider.common.external_connections</code>, and can be used when configuring your Great Expectations data source within the <code>configure_batch_definition</code> or <code>configure_checkpoint</code> function.</p> <p>Here is an example that uses the <code>build_snowflake_key_connection</code> function to connect to Snowflake using a private key.</p> <pre><code>from great_expectations_provider.operators.validate_batch import GXValidateBatchOperator\nfrom great_expectations_provider.common.external_connections import (\n   build_snowflake_key_connection\n)\n\nimport great_expectations as gx\nfrom great_expectations.core.batch_definition import BatchDefinition\nfrom great_expectations.data_context import AbstractDataContext\n\n\ndef my_batch_definition_function(context: AbstractDataContext) -&gt; BatchDefinition:\n   snowflake_config = build_snowflake_key_connection(conn_id=\"snowflake_conn_id\")\n   return (\n      context.data_sources.add_snowflake(\n         name=\"snowflake sandbox\",\n         account=snowflake_config.account,\n         user=snowflake_config.user,\n         role=snowflake_config.role,\n         password=\"&lt;PLACEHOLDER PASSWORD&gt;\",  # must be provided to pass validation but will be ignored\n         warehouse=snowflake_config.warehouse,\n         database=snowflake_config.database,\n         schema=snowflake_config.schema,\n         kwargs={\"private_key\": snowflake_config.private_key},\n      )\n      .add_table_asset(name=\"&lt;SCHEMA.TABLE&gt;\", table_name=\"&lt;TABLE_NAME&gt;\")\n      .add_batch_definition_whole_table(  # you can also batch by year, month, or day here\n         name=\"Whole Table Batch Definition\"\n      )\n   )\n\n\nmy_expectations = gx.ExpectationSuite(\n   name=\"&lt;SUITE_NAME&gt;\",\n   expectations=[\n      # define expectations\n   ],\n)\n\nmy_batch_operator = GXValidateBatchOperator(\n   task_id=\"my_batch_operator\",\n   configure_batch_definition=my_batch_definition_function,\n   expect=my_expectations,\n   context_type=\"ephemeral\",\n)\n</code></pre>"},{"location":"contributing/code-of-conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"contributing/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socioeconomic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"contributing/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"contributing/code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned with this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"contributing/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"contributing/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at humans@astronomer.io.</p> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"contributing/code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"contributing/code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"contributing/code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"contributing/code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"contributing/code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"contributing/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ. Translations are available at this page.</p>"},{"location":"contributing/contributing-guide/","title":"Contributing guide","text":"<p>All contributions, bug reports, bug fixes, documentation improvements, and enhancements are welcome.</p> <p>All contributors and maintainers to this project should abide by the Contributor Code of Conduct.</p> <p>Learn more about the contributors' roles in the Roles page.</p> <p>This document describes how to contribute to the Great Expectations Airflow Provider, covering:</p> <ul> <li>Overview of how to contribute</li> <li>How to set up the local development environment</li> <li>Running tests</li> <li>Authoring the documentation</li> </ul>"},{"location":"contributing/contributing-guide/#overview-of-how-to-contribute","title":"Overview of how to contribute","text":"<p>To contribute to the Great Expectations Airflow Provider project:</p> <ol> <li>Create a GitHub Issue describing a bug, enhancement, or feature request.</li> <li>Fork the repository.</li> <li>In your fork, open a branch off of the <code>main</code> branch.</li> <li>Create a Pull Request into the <code>main</code> branch of the Provider repo from your forked feature branch.</li> <li>Link your issue to the Pull Request.</li> <li>After you complete development on your feature branch, request a review. A maintainer will merge your PR after all reviewers approve it.</li> </ol>"},{"location":"contributing/contributing-guide/#set-up-a-local-development-environment","title":"Set up a local development environment","text":"<p>Setting up a local development environment involves fulfilling requirements, getting a copy of the repository, and setting up a virtual environment.</p>"},{"location":"contributing/contributing-guide/#requirements","title":"Requirements","text":"<ul> <li>Git</li> <li>Python version 3.9 to 3.12</li> <li>Great Expectations version 1.3.11+</li> <li>Apache Airflow\u00ae version 2.1.0+</li> </ul>"},{"location":"contributing/contributing-guide/#get-a-copy-of-the-repository","title":"Get a copy of the repository","text":"<ol> <li> <p>Fork the Provider repository.</p> </li> <li> <p>Clone your fork.    <pre><code>git clone https://github.com/my-user/airflow-provider-great-expectations.git\n</code></pre></p> </li> </ol>"},{"location":"contributing/contributing-guide/#set-up-a-virtual-environment","title":"Set up a virtual environment","text":"<p>You can use any virtual environment tool. The following example uses the <code>venv</code> tool included in the Python standard library.</p> <ol> <li> <p>Create the virtual environment.    <pre><code>$ python -m venv .venv\n</code></pre></p> </li> <li> <p>Activate the virtual environment.    <pre><code>$ . .venv/bin/activate\n</code></pre></p> </li> <li> <p>Install the package and testing dependencies.    <pre><code>(.venv) $ pip install -e '.[tests]'\n</code></pre></p> </li> </ol>"},{"location":"contributing/contributing-guide/#run-tests","title":"Run tests","text":"<p>Test with <code>pytest</code>:</p> <ol> <li>Install <code>pytest</code> as a dependency.    <pre><code>pip install \"airflow-provider-great-expectations[tests]\"\n</code></pre></li> <li>Run the following command, which will provide a concise output when all tests pass and minimum necessary details when they don't.    <pre><code>pytest -p no:warnings\n</code></pre>    The <code>no:warnings</code> flag filters out deprecation messages that may be issued by Airflow.</li> </ol>"},{"location":"contributing/contributing-guide/#write-docs","title":"Write docs","text":"<p>We use Markdown to author Great Expectations Airflow Provider documentation. We use hatch to build and release the docs.</p> <ol> <li>Update Markdown files in the <code>docs/</code> folder.</li> <li>Build and serve the documentation locally to preview your changes.    <pre><code>hatch run docs:dev\n</code></pre></li> <li>Open an issue and PR for your changes.</li> <li>Once approved, release the documentation with the current project version and set it to the latest.    <pre><code>hatch run docs:gh-release\n</code></pre></li> </ol>"},{"location":"contributing/contributor-roles/","title":"Contributor roles","text":"<p>Contributors are welcome and are greatly appreciated! Every little bit helps, and we give credit to them.</p> <p>This document aims to explain the current roles in the Great Expectations Airflow Provider project. For more information, check the contributing guide.</p>"},{"location":"contributing/contributor-roles/#contributors","title":"Contributors","text":"<p>A contributor is anyone who wants to contribute code, documentation, tests, ideas, or anything to the Great Expectations Airflow Provider project.</p> <p>Great Expectations Airflow Provider contributors are listed in the GitHub insights page.</p> <p>Contributors are responsible for:</p> <ul> <li>Fixing bugs</li> <li>Refactoring code</li> <li>Improving processes and tooling</li> <li>Adding features</li> <li>Improving the documentation</li> </ul>"},{"location":"contributing/contributor-roles/#committers","title":"Committers","text":"<p>Committers are community members with write access to the Great Expectations Airflow Provider GitHub repository. They can modify the code and the documentation and accept others' contributions to the repo.</p> <p>Check contributors for the official list of Great Expectations Airflow Provider committers.</p> <p>Committers have the same responsibilities as standard contributors and also perform the following actions:</p> <ul> <li>Reviewing &amp; merging pull-requests</li> <li>Scanning and responding to GitHub issues, helping triaging them</li> </ul> <p>If you know you are not going to be able to contribute for a long time (for instance, due to a change of job or circumstances), you should inform other maintainers, and we will mark you as \"emeritus\". Emeritus committers will no longer have write access to the repo. As merit earned never expires, once an emeritus committer becomes active again, they can email a maintainer from Astronomer and ask to be reinstated.</p>"},{"location":"contributing/contributor-roles/#prerequisites-to-becoming-a-committer","title":"Prerequisites to becoming a committer","text":"<p>General prerequisites that we look for in all candidates:</p> <ol> <li>Consistent contribution over the last few months</li> <li>Visibility on discussions on the Slack channel or GitHub issues/discussions</li> <li>Contributes to community health and project's sustainability for the long term</li> <li>Understands the project's contributors' guidelines</li> </ol> <p>Astronomer is responsible and accountable for releasing new versions of the Great Expectations Airflow Provider in PyPI, following the milestones. Astronomer has the right to grant and revoke write access permissions to the project's official repository for any reason it sees fit.</p>"},{"location":"contributing/contributors/","title":"Contributors","text":"<p>There are different ways people can contribute to the Great Expectations Airflow Provider. Learn more about the project contributors roles.</p>"},{"location":"contributing/contributors/#committers","title":"Committers","text":"<ul> <li>Pankaj Koti (@pankajkoti)</li> <li>Joshua Stauffer (@joshua-stauffer)</li> <li>Tyler Hoffman (@tyler-hoffman)</li> </ul>"},{"location":"contributing/contributors/#contributors_1","title":"Contributors","text":"<p>Find more contributors in our GitHub page.</p>"}]}