name: "Integration Tests"

on:
  push: # Run on pushes to the default branch
    branches: [main]
  pull_request: # Also run on pull requests originated from forks
    branches: [main]

# This allows a subsequently queued workflow run to interrupt and cancel previous runs
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true


jobs:
  Authorize:
    environment: ${{ github.event_name == 'pull_request' &&
      github.event.pull_request.head.repo.full_name != github.repository &&
      'external' || 'internal' }}
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - run: true

  integration-test:
    needs: Authorize
    runs-on: ubuntu-latest
    permissions:
      contents: read

    env:
      GX_CLOUD_ORGANIZATION_ID: ${{ secrets.GX_CLOUD_ORGANIZATION_ID }}
      GX_CLOUD_ACCESS_TOKEN: ${{ secrets.GX_CLOUD_ACCESS_TOKEN }}
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
      POSTGRES_PORT: 5433  # Using 5433 to avoid conflict with local postgres

    services:
      postgres:
        image: postgres:13
        ports:
          - 5433:5432
        env:
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          POSTGRES_DB: ${{ env.POSTGRES_DB }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install uv
        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3
        with:
          version: "0.7.13"

      - name: Install dependencies
        run: uv pip install --system -e ".[postgresql,tests]"

      - name: Setup
        run: |
          airflow db reset -y
          airflow db migrate

      - name: Run Regular Integration Tests
        run: pytest -vvv -m integration tests/integration

  spark-integration-test:
    needs: Authorize
    runs-on: ubuntu-latest
    permissions:
      contents: read

    env:
      GX_CLOUD_ORGANIZATION_ID: ${{ secrets.GX_CLOUD_ORGANIZATION_ID }}
      GX_CLOUD_ACCESS_TOKEN: ${{ secrets.GX_CLOUD_ACCESS_TOKEN }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Start services
        run: |
          docker compose -f docker/spark/docker-compose.yml up -d --quiet-pull --wait --wait-timeout 90

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install uv
        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3
        with:
          version: "0.7.13"

      - name: Install dependencies
        run: uv pip install --system  -e ".[tests,spark]"

      - name: Setup
        run: |
          airflow db reset -y
          airflow db migrate

      - name: Run Spark Integration Tests
        run: pytest -vvv -m spark_integration tests/integration

  spark-connect-integration-test:
    needs: Authorize
    runs-on: ubuntu-latest
    permissions:
      contents: read

    env:
      GX_CLOUD_ORGANIZATION_ID: ${{ secrets.GX_CLOUD_ORGANIZATION_ID }}
      GX_CLOUD_ACCESS_TOKEN: ${{ secrets.GX_CLOUD_ACCESS_TOKEN }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Start services
        run: |
          docker compose -f docker/spark/docker-compose.yml up -d --quiet-pull --wait --wait-timeout 90

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install uv
        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3
        with:
          version: "0.7.13"

      - name: Install dependencies
        run: uv pip install --system -e ".[tests,spark]"

      - name: Setup
        run: |
          airflow db reset -y
          airflow db migrate

      - name: Run Spark Connect Integration Tests
        run: pytest -vvv -m spark_connect_integration tests/integration
