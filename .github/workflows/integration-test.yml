name: "Integration Tests"

on:
  pull_request:
  push:
    branches:
      - main

jobs:
  integration-test:
    runs-on: ubuntu-latest

    env:
      GX_CLOUD_ORGANIZATION_ID: ${{secrets.GX_CLOUD_ORGANIZATION_ID}}
      GX_CLOUD_ACCESS_TOKEN: ${{secrets.GX_CLOUD_ACCESS_TOKEN}}
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
      POSTGRES_PORT: 5433  # Using 5433 to avoid conflict with local postgres

    services:
      postgres:
        image: postgres:13
        ports:
          - 5433:5432
        env:
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          POSTGRES_DB: ${{ env.POSTGRES_DB }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Upgrade pip
        run: python -m pip install --upgrade pip

      - name: Install Library
        run: pip install .[postgresql,tests]

      - name: Setup
        run: |
          airflow db reset -y
          airflow db init

      - name: Run Regular Integration Tests
        run: pytest -vvv -m integration tests/integration

  spark-integration-test:
    runs-on: ubuntu-latest

    env:
      GX_CLOUD_ORGANIZATION_ID: ${{secrets.GX_CLOUD_ORGANIZATION_ID}}
      GX_CLOUD_ACCESS_TOKEN: ${{secrets.GX_CLOUD_ACCESS_TOKEN}}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Start services
        run: |
          docker compose -f docker/spark/docker-compose.yml up -d --quiet-pull --wait --wait-timeout 90

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Upgrade pip
        run: python -m pip install --upgrade pip

      - name: Install Library
        run: pip install .[tests,spark]

      - name: Setup
        run: |
          airflow db reset -y
          airflow db init

      - name: Run Spark Integration Tests
        run: pytest -vvv -m spark_integration tests/integration

  spark-connect-integration-test:
    runs-on: ubuntu-latest

    env:
      GX_CLOUD_ORGANIZATION_ID: ${{secrets.GX_CLOUD_ORGANIZATION_ID}}
      GX_CLOUD_ACCESS_TOKEN: ${{secrets.GX_CLOUD_ACCESS_TOKEN}}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Start services
        run: |
          docker compose -f docker/spark/docker-compose.yml up -d --quiet-pull --wait --wait-timeout 90

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Upgrade pip
        run: python -m pip install --upgrade pip

      - name: Install Library
        run: pip install .[tests,spark]

      - name: Setup
        run: |
          airflow db reset -y
          airflow db init

      - name: Run Spark Connect Integration Tests
        run: pytest -vvv -m spark_connect_integration tests/integration
