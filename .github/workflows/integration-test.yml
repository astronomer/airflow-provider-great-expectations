name: "Integration Tests"

on:
  pull_request:
  push:
    branches:
      - main

jobs:
  integration-test:
    runs-on: ubuntu-latest

    env:
      GX_CLOUD_ORGANIZATION_ID: ${{secrets.GX_CLOUD_ORGANIZATION_ID}}
      GX_CLOUD_ACCESS_TOKEN: ${{secrets.GX_CLOUD_ACCESS_TOKEN}}
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
      POSTGRES_PORT: 5433  # Using 5433 to avoid conflict with local postgres

    services:
      postgres:
        image: postgres:13
        ports:
          - 5433:5432
        env:
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          POSTGRES_DB: ${{ env.POSTGRES_DB }}

      spark:
        image: bitnami/spark:3.3.2
        ports:
          - "9090:8080"
          - "7077:7077"
      spark-connect:
        image: bitnami/spark:3.5.2
        ports:
          - "15002:15002"
        # See https://spark.apache.org/docs/latest/spark-connect-overview.html#download-and-start-spark-server-with-spark-connect
        command: ./sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.5.2
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Start services
        run: |
          docker compose -f tests/docker/spark/docker-compose.yml up -d --quiet-pull --wait --wait-timeout 90

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10.14"

      - name: Upgrade pip
        run: python -m pip install --upgrade pip

      - name: Install Library
        run: pip install .

      - name: Install Dependencies
        run: pip install -r test-requirements.txt

      - name: Setup
        run: |
          airflow db reset -y
          airflow db init

      - name: Run Unit Tests
        run: pytest -vvv tests/integration/test_validate_dataframe_operator.py::TestGXValidateDataFrameOperator
